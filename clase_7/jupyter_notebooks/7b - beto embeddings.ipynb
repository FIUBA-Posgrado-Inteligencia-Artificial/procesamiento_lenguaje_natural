{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63Jpt-wYcJ3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Beto\n",
        "[GitHub LINK](https://github.com/dccuchile/beto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcPiEBdt8NqM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvoZ8YlK0vOQ"
      },
      "source": [
        "## 1 - BETO embeddings\n",
        "Se necesita instalar la librería de \"transformers\" de Hugging Face para utilizar los modelos de BERT y sus funciones de ayuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vQGeFm0pdn1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJHhebCnnvnn"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel, BertTokenizer\n",
        "\n",
        "# Muy importante que para tensorflow los modelos Bert deben empezar con \"TF\"\n",
        "# de lo contrario estaremos utilizando un modelo para pytorch\n",
        "\n",
        "# Descargamos el modelo base de BETO y su correspondiente tokenizer (BERT para español)\n",
        "model = TFBertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmg5ZuNb1Bh6"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsAss9idk9Oy"
      },
      "outputs": [],
      "source": [
        "max_length = 12\n",
        "text = \"hola mundo! soy beto\"\n",
        "\n",
        "input_dict = tokenizer(text,\n",
        "                       add_special_tokens=True,\n",
        "                       return_token_type_ids=True, # indican segmentación de dos textos como por ejemplo para entailment\n",
        "                       return_attention_mask=True,\n",
        "                       max_length=max_length, \n",
        "                       padding=\"max_length\", \n",
        "                       truncation=True, \n",
        "                       return_tensors='tf')\n",
        "\n",
        "# Idem con encode plus (mismo resultado)\n",
        "# input_dict = tokenizer.encode_plus(\n",
        "#     text,\n",
        "#     add_special_tokens=True,\n",
        "#     return_token_type_ids=False,\n",
        "#     return_attention_mask=True,\n",
        "#     max_length=max_length, # truncates if len(s) > max_length\n",
        "#     padding=\"max_length\",\n",
        "#     truncation=True,\n",
        "#     return_tensors='tf'\n",
        "# )\n",
        "\n",
        "print(input_dict.keys())\n",
        "print(\"Inputs ids:\", input_dict['input_ids'])\n",
        "print(\"Attention mask:\", input_dict['attention_mask'])\n",
        "print(\"Token type ids:\", input_dict['token_type_ids'])\n",
        "\n",
        "# EL primer token es el de CLS\n",
        "# EL ante último token es de SEP\n",
        "# EL último token es de PAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY2Pk1U3mg2e"
      },
      "source": [
        "Se puede observar que el sistema está creando más tokens que palabras, esto es porque BERT agerga tokens especiales semánticos (separadores, conjugación, símbolos, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwaq1y8gzMTe"
      },
      "outputs": [],
      "source": [
        "# Tokens transformados a Ids\n",
        "print(input_dict['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGXF63u0pBmv"
      },
      "outputs": [],
      "source": [
        "# Ids transformamos a tokens\n",
        "for id in input_dict['input_ids'][0]:\n",
        "    token = tokenizer.convert_ids_to_tokens(int(id))\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDxrjDkqrXSK"
      },
      "source": [
        "__IMPORTANTE:__ \"beto\" no pudo ser tokenizada (no estaba en el vocabulario) por lo que el tokenizar la dividió en 2 palabras tokenizables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fcKsoS6pks2"
      },
      "outputs": [],
      "source": [
        "# Convertir todos los tokens descartando los especiales\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_dict['input_ids'][0], skip_special_tokens=True) \n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOIPT2wwmxXQ"
      },
      "outputs": [],
      "source": [
        "# Inferir con el modelo de Bert, nótese que el input es el conjunto de Ids y attention mask\n",
        "X_ensayo = [input_dict['input_ids'], input_dict['attention_mask']]\n",
        "out = model.predict(X_ensayo)\n",
        "last_hidden_state, pooler_output = out[0], out[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byPMhnPmnITP"
      },
      "outputs": [],
      "source": [
        "# Embedding de salida que representa toda la sentencia de entrada:\n",
        "pooler_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eB00qLfnlzI"
      },
      "outputs": [],
      "source": [
        "# Embedding de cada palabra/token de entrada\n",
        "# (batch_size, sequence_length, hidden_size)\n",
        "print(\"Embeddings shape:\", last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S15QvZYsvDb8"
      },
      "source": [
        "## 2 - Interpretar cómo la tokenización transforma las palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYlpgnQuqB5c"
      },
      "outputs": [],
      "source": [
        "text2 = [\"Acercame el banco para sentarme\", \n",
        "         \"Esto lo encontré debajo del banco de una plaza\", \n",
        "         \"Hoy tengo que ir al banco a hacer unos trámites\",\n",
        "         \"Esa plata me la depositan directamente en el banco\",\n",
        "         \"Yo no me banco que me hagan eso\",\n",
        "         \"A ella la banco en lo que sea\"]\n",
        "\n",
        "input_dict2 = tokenizer(text2,\n",
        "                    add_special_tokens=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_attention_mask=True,\n",
        "                    max_length=max_length, \n",
        "                    padding=\"max_length\", \n",
        "                    truncation=True, \n",
        "                    return_tensors='tf')\n",
        "\n",
        "print(input_dict2.keys())\n",
        "print(\"Inputs ids:\", input_dict2['input_ids'])\n",
        "print(\"Attention mask:\", input_dict2['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne0CG21Hq7o3"
      },
      "outputs": [],
      "source": [
        "# Convertir todos los tokens descartando los especiales\n",
        "for in_dict in input_dict2['input_ids']:\n",
        "    print(tokenizer.convert_ids_to_tokens(in_dict, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDw4iHPbs_Rp"
      },
      "outputs": [],
      "source": [
        "X_ensayo2 = [input_dict2['input_ids'], input_dict2['attention_mask']]\n",
        "out2 = model.predict(X_ensayo2)\n",
        "last_hidden_state2, pooler_output2 = out2[0], out2[1]\n",
        "last_hidden_state2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxqSSLBxMvX-"
      },
      "outputs": [],
      "source": [
        "# El embedding de está es:\n",
        "last_hidden_state2[0, 1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rym7tfo-vIfw"
      },
      "source": [
        "## 3 - Comparar los embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25WGpGXvuxj5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise\n",
        "\n",
        "def compare_embeddings(set_idx):\n",
        "    embs = []\n",
        "    words = []\n",
        "    for i in range(len(last_hidden_state2)):\n",
        "        token_id = int(input_dict2['input_ids'][i][set_idx[i]])\n",
        "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "        words.append(token)\n",
        "        embs.append(last_hidden_state2[i, set_idx[i], :])\n",
        "\n",
        "    similarity_cosine = pairwise.cosine_similarity(embs)\n",
        "\n",
        "    fig = plt.figure(figsize=(16,9))\n",
        "    ax = fig.add_subplot()\n",
        "    sns.heatmap(similarity_cosine, xticklabels=words, yticklabels=words,\n",
        "     annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, mask=np.triu(similarity_cosine))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# el índice para cada texto de `text2` en donde se encuentra la palabra \"banco\"\n",
        "set_idx = [4,6,6,10,4,4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EGifGhRtLrm"
      },
      "outputs": [],
      "source": [
        "# Obtener la matriz de distancia coseno entre los embeddings \n",
        "compare_embeddings(set_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpDlOZ3TrllM"
      },
      "source": [
        "## Conclusiones \n",
        "- Como estamos utilizando BERT \"uncased\" el sistema pasa a minúsculas al texto automaticamente. Debemos usar BETO cased para soportar mayúsculas.\n",
        "- BERT soporta utilizar palabras con tílde y posee un token_id diferente para cada una.\n",
        "- BERT no es muy bueno para obtener embeddings individuales de palabras (ELMo es mejor en ese aspecto o Glove/Fasttext). Lo utilizaremos para obtener el embedding de la sentencia o contexto para clasificación o comparación de textos."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "7b - beto embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

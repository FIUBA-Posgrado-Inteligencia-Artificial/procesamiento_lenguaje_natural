{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGfZ771blhgk"
      },
      "source": [
        "# Arquitecturas aplicadas a clasificación de texto\n",
        "\n",
        "Utilizaremos el dataset 20 Newsgroups para probar los modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvmO-ANtnKTv"
      },
      "outputs": [],
      "source": [
        "import os, re, csv, math, codecs, logging\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "import gdown\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDB1L0M5m8ar"
      },
      "outputs": [],
      "source": [
        "# cargamos 20 Newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H0JHHsyKYRh"
      },
      "outputs": [],
      "source": [
        "# descargamos los embeddings de palabras de Fasttext para inglés y descomprimimos el archivo.\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6Q2psVeNaM4"
      },
      "outputs": [],
      "source": [
        "# cargamos los embeddings de palabras\n",
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "\n",
        "for line in f:\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PVbAh-fnP4E"
      },
      "outputs": [],
      "source": [
        "print(newsgroups_train.data[16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRBZtI2zoTT7"
      },
      "outputs": [],
      "source": [
        "# instanciamos el tokenizador\n",
        "token = Tokenizer(num_words=30000,\n",
        "                filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                lower=True,\n",
        "                split=' ',\n",
        "                char_level=False,\n",
        "                oov_token=\"UNK\",\n",
        "                document_count=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClA5L6pnpHP7"
      },
      "outputs": [],
      "source": [
        "# fiteamos el tokenizador\n",
        "token.fit_on_texts(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reverse_dictionary = token.index_word\n",
        "dictionary = dict([(value, key) for (key, value) in reverse_dictionary.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUgV6okQN_op"
      },
      "outputs": [],
      "source": [
        "# cargamos en una matriz los embeddings de las palabras\n",
        "# presentes en el vocabulario\n",
        "embed_dim=300\n",
        "num_words=len(dictionary)+1\n",
        "embedding_matrix=np.zeros([num_words,embed_dim])\n",
        "for word, idx in dictionary.items():\n",
        "  if idx <= num_words and word in embeddings_index:\n",
        "    embedding_matrix[idx,:]=embeddings_index[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Wcf-ifpjjM"
      },
      "outputs": [],
      "source": [
        "# se tokenizan los textos\n",
        "train_sequences=token.texts_to_sequences(newsgroups_train.data)\n",
        "test_sequences=token.texts_to_sequences(newsgroups_test.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2toIg26nN8d7"
      },
      "outputs": [],
      "source": [
        "train_sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oaqzg4Ayp-r"
      },
      "outputs": [],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhu1qis2PdXW"
      },
      "source": [
        "En este punto seleccionamos el tamaño de contexto a procesar en la variable `max_len`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEiPTqOBT_V7"
      },
      "outputs": [],
      "source": [
        "max_len=500\n",
        "train_sequences=pad_sequences(train_sequences,maxlen=max_len)\n",
        "test_sequences=pad_sequences(test_sequences,maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJlUrLkfsBfK"
      },
      "outputs": [],
      "source": [
        "train_sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWNt5zTlqZv9"
      },
      "outputs": [],
      "source": [
        "dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmflnp5NQF8g"
      },
      "outputs": [],
      "source": [
        "token.index_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87VkHIk0idn2"
      },
      "source": [
        "# Suma de embeddings + MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGsJgyDDiicu"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D,\\\n",
        "                         Dropout, Dense, Lambda, Concatenate, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPLcPIS--Jgp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class F1Callback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    Además implementa la finalización del entrenamiento (Early Stopping)\n",
        "    si la perplejidad no mejora después de `patience` epochs.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, X_val,y_val,num_classes, history_f1, patience=5):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.X_val = X_val\n",
        "      self.y_val = y_val\n",
        "\n",
        "      self.max_score = 0\n",
        "      self.num_classes = num_classes\n",
        "      self.epsilon = 10E-8\n",
        "      self.patience = patience\n",
        "      self.patience_counter = 0\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        predictions = self.model.predict(self.X_val,verbose=0)\n",
        "\n",
        "        y_pred = np.argmax(predictions,axis=1)\n",
        "\n",
        "        counter = np.zeros((self.num_classes,self.num_classes))\n",
        "\n",
        "        for idx_pred,idx_true in zip(y_pred,self.y_val):\n",
        "          counter[idx_pred,idx_true] += 1\n",
        "\n",
        "        # sea calcula TP, FN y FP\n",
        "        TP = np.diag(counter)\n",
        "        FN = counter.sum(axis=0)-TP\n",
        "        FP = counter.sum(axis=1)-TP\n",
        "\n",
        "        precision = TP/(TP+FP+self.epsilon)\n",
        "        recall = TP/(TP+FN+self.epsilon)\n",
        "\n",
        "        # se calcula el F1-sscore\n",
        "        f1 = 2*precision*recall/(precision+recall+self.epsilon)\n",
        "\n",
        "        current_score = np.mean(f1)\n",
        "\n",
        "        history_f1.append(current_score)\n",
        "\n",
        "        print(f'\\n f1 macro: {current_score} \\n')\n",
        "\n",
        "        # chequeamos si tenemos que  detener el entrenamiento\n",
        "        if current_score > self.max_score:\n",
        "          self.max_score = current_score\n",
        "          self.model.save(\"my_model.keras\")\n",
        "          print(\"Saved new model!\")\n",
        "          self.patience_counter = 0\n",
        "        else:\n",
        "          self.patience_counter += 1\n",
        "          if self.patience_counter == 5:\n",
        "            print(\"Stopping training...\")\n",
        "            self.model.stop_training = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlOr3GDayiAK"
      },
      "outputs": [],
      "source": [
        "nb_words=num_words\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
        "model.add(Lambda(lambda x: K.sum(x, axis=1)))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(20, activation='softmax'))  #multi-label (k-hot encoding)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAhKga5rjAHG"
      },
      "outputs": [],
      "source": [
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=64,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnmjFl8T6jyr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ane8TKiv6U9l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_f1) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_f1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8QnWlpxyqzF"
      },
      "source": [
        "## Clasificador Embeddings + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjoQz25NazxB"
      },
      "outputs": [],
      "source": [
        "nb_words=num_words\n",
        "num_filters=64\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
        "\n",
        "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(2))\n",
        "\n",
        "model.add(Conv1D(num_filters*2, 7, activation='relu', padding='same'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(20, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnkm8psrbO3A"
      },
      "outputs": [],
      "source": [
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=128,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_G07HdAzwlr"
      },
      "source": [
        "## Clasificación con TextCNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCaQWMIVdSiI"
      },
      "outputs": [],
      "source": [
        "\n",
        "nb_words=num_words\n",
        "num_filters=64\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
        "\n",
        "conv4=Conv1D(num_filters, 4, activation='relu', padding='same')(embedding_layer)\n",
        "conv3=Conv1D(num_filters, 3, activation='relu', padding='same')(embedding_layer)\n",
        "conv2=Conv1D(num_filters, 2, activation='relu', padding='same')(embedding_layer)\n",
        "pool4=GlobalMaxPooling1D()(conv4)\n",
        "pool3=GlobalMaxPooling1D()(conv3)\n",
        "pool2=GlobalMaxPooling1D()(conv2)\n",
        "added = Concatenate()([pool4, pool3, pool2])\n",
        "\n",
        "dense1=Dense(32, activation='relu')(added)\n",
        "dense2=Dense(20, activation='softmax')(dense1)\n",
        "\n",
        "model=Model(input_layer , dense2)\n",
        "\n",
        "# adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6YLe-vn1GX3"
      },
      "outputs": [],
      "source": [
        "\n",
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=128,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KTF5uIW7QAe"
      },
      "source": [
        "# MLP + Embeddings + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqvbWeXlz-qP"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dot,RepeatVector,TimeDistributed,Multiply,Lambda,Flatten,Activation,Reshape\n",
        "from keras.activations import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lQCd7N7Uwu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def softMaxOverTime(x):\n",
        "    return softmax(x,axis=1)\n",
        "\n",
        "key_dim=50\n",
        "nb_words=num_words\n",
        "num_filters=64\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
        "\n",
        "dense_input = Dense(key_dim, activation=\"tanh\")(embedding_layer)\n",
        "ulog_attention = Dense(1,activation=\"linear\")(dense_input)\n",
        "\n",
        "attention = Activation(softMaxOverTime)(ulog_attention)\n",
        "\n",
        "repeated_attention = TimeDistributed(RepeatVector(embed_dim))(attention)\n",
        "\n",
        "repeated_attention = Reshape([max_len,embed_dim])(repeated_attention)\n",
        "\n",
        "weighted_embeddings = Multiply()([repeated_attention,embedding_layer])\n",
        "embedding_sum = Lambda(lambda x: K.sum(x, axis=1),output_shape=(300,))(weighted_embeddings)\n",
        "\n",
        "dense1=Dense(32, activation='relu')(embedding_sum)\n",
        "dense2=Dense(20, activation='softmax')(dense1)\n",
        "\n",
        "model=Model(input_layer , dense2)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_6eY80XIe48"
      },
      "outputs": [],
      "source": [
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=128,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEucYPxZJMCk"
      },
      "source": [
        "# MLP + Embeddings + Attention + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkvAI3wcJRmX"
      },
      "outputs": [],
      "source": [
        "\n",
        "value_dim=100\n",
        "\n",
        "def softMaxOverTime(x):\n",
        "    return softmax(x,axis=1)\n",
        "\n",
        "\n",
        "nb_words=num_words\n",
        "num_filters=64\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
        "\n",
        "conv_out=Conv1D(value_dim,8,padding=\"same\")(embedding_layer)\n",
        "conv_out=Activation(\"relu\")(conv_out)\n",
        "#conv_out=Conv1D(value_dim,8,activation=\"relu\",padding=\"same\")(conv_out)\n",
        "conv_out=Conv1D(value_dim,8,activation=\"tanh\",padding=\"same\")(conv_out)\n",
        "\n",
        "ulog_attention=Dense(1,activation=\"linear\")(conv_out)\n",
        "attention=Activation(softMaxOverTime)(ulog_attention)\n",
        "repeated_attention=TimeDistributed(RepeatVector(value_dim))(attention)\n",
        "repeated_attention=Reshape([max_len,value_dim])(repeated_attention)\n",
        "weighted_embeddings=Multiply()([repeated_attention,conv_out])\n",
        "embedding_sum = Lambda(lambda x: K.sum(x, axis=1),output_shape=(100,))(weighted_embeddings)\n",
        "\n",
        "dense1=Dense(100, activation='relu')(embedding_sum)\n",
        "dense2=Dense(20, activation='softmax')(dense1)\n",
        "\n",
        "model=Model(input_layer , dense2)\n",
        "\n",
        "# adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxm9Eu3rJ4gs"
      },
      "outputs": [],
      "source": [
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=128,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLHo1NKmMe3S"
      },
      "source": [
        "# Bidir RNN + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcOrcMWzMhoW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Bidirectional, LSTM\n",
        "\n",
        "value_dim=100\n",
        "\n",
        "def softMaxOverTime(x):\n",
        "    return softmax(x,axis=1)\n",
        "\n",
        "nb_words=num_words\n",
        "num_filters=64\n",
        "\n",
        "input_layer = Input(shape=(max_len,))\n",
        "embedding_layer=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
        "# lstm_out=Bidirectional(LSTM(value_dim, return_sequences=True))(embedding_layer)\n",
        "# lstm_out=Bidirectional(LSTM(value_dim, return_sequences=True))(lstm_out)\n",
        "lstm_out=Bidirectional(LSTM(value_dim, return_sequences=True,activation=\"relu\"),merge_mode=\"sum\")(embedding_layer)\n",
        "\n",
        "ulog_attention=Dense(1,activation=\"linear\")(lstm_out)\n",
        "attention=Activation(softMaxOverTime)(ulog_attention)\n",
        "repeated_attention=TimeDistributed(RepeatVector(value_dim))(attention)\n",
        "repeated_attention=Reshape([max_len,value_dim])(repeated_attention)\n",
        "weighted_embeddings=Multiply()([repeated_attention,lstm_out])\n",
        "embedding_sum = Lambda(lambda x: K.sum(x, axis=1),output_shape=(None,value_dim))(weighted_embeddings)\n",
        "\n",
        "dense1=Dense(100, activation='relu')(embedding_sum)\n",
        "dense2=Dense(20, activation='softmax')(dense1)\n",
        "\n",
        "model=Model(input_layer , dense2)\n",
        "\n",
        "# adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88dnPOJVNrLS"
      },
      "outputs": [],
      "source": [
        "history_f1 = []\n",
        "model.fit(train_sequences, newsgroups_train.target,batch_size=128,epochs=40,callbacks=[F1Callback(test_sequences,newsgroups_test.target,20,history_f1)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

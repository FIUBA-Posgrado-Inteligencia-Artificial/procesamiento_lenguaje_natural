{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3a - Embeddings Glove y Fasttext.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHvfSLeC5QShozlRS4xQ6+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I3Ua7AcJXvEW"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Word Embeddings con Glove y Fasttext"]},{"cell_type":"markdown","metadata":{"id":"V64c1mLJeQFq"},"source":["Este código permite levantar los embeddings de Fastex y de Glove y poder utilizarlos para calcular la distancia entre palabras"]},{"cell_type":"code","source":["!pip install --upgrade --no-cache-dir gdown --quiet"],"metadata":{"id":"t9yRAgghhTtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5U2K2RERweIY"},"source":["import logging\n","\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PoahGjfGX1vN"},"source":["### Datos"]},{"cell_type":"code","metadata":{"id":"HvaLePMwpBjV"},"source":["# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n","# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n","# disponibles descargar de la página oficial como se explica en el siguiente bloque\n","import os\n","import gdown\n","if os.access('fasttext.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KU5qmAYh3LATMvVgocFDfW-PK3prm1WU'\n","    output = 'fasttext.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"Los embeddings fasttext.pkl ya están descargados\")\n","\n","if os.access('gloveembedding.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n","    output = 'gloveembedding.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0RVEeHfzbg3"},"source":["# De donde se obtuvieron estos embeddings:\n","\n","# Link de donde se dscargo los embeddings de Glove (de ese zip solo se sube el embedding de 50d)\n","# http://nlp.stanford.edu/data/glove.twitter.27B.zip\n","# Link de descarga de los embeddings 300 de FastText\n","# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCuwnSJn7WPF"},"source":["# Paper de Glove\n","# https://nlp.stanford.edu/pubs/glove.pdf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTNWSWVWaBno"},"source":["### 1 - Ensayar los embeddings de Glove y Fasttext"]},{"cell_type":"code","metadata":{"id":"a9HDjEUNd4ff"},"source":["class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dicctionarios para traducir de embedding a IDX de la palabra\n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    WORD_MAX_SIZE = 60\n","\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GiCtPUYirc1t"},"source":["# Creamos los generadores de embeddings\n","# Puede demorar 1 minuto porque tiene que levantar los archivos de embeddings\n","model_fasttext = FasttextEmbeddings()\n","model_glove = GloveEmbeddings()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQKIDrCWfuxV"},"source":["def plot_matrix_distance(words, dist):\n","    fig = plt.figure(figsize=(16,9))\n","    ax = fig.add_subplot()\n","    sns.heatmap(dist, xticklabels=words, yticklabels=words, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCBmo0npei9x"},"source":["# Ojo que si usan scipy la distancia coseno no es la similitud coseno (esta \"negada\")\n","from sklearn.metrics import pairwise\n","\n","def embeddings_matrix_distance(model, words):\n","    print(\"Cantidad de palabras:\", len(words))\n","    emb = model.get_words_embeddings(words)\n","    print(\"Dimensiones de los embeddings:\", emb.shape)\n","    dist = pairwise.cosine_similarity(emb, emb)\n","    plot_matrix_distance(words, dist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MlZWL1KaQq2"},"source":["# Palabras a ensayar\n","# Algunas relativas con saludos y otras con dispositivos\n","words = [\"hi\", \"hello\", \"bye\", \"goodbye\", \"morning\", \"computer\", \"machine\", \"laptop\", \"device\", \"printer\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnrchHWMZ5-p"},"source":["embeddings_matrix_distance(model_glove, words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hfg7_QxaYBf"},"source":["embeddings_matrix_distance(model_fasttext, words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EOxqbPg7b52U"},"source":["### 2 - Conclusión hasta el momento\n","Como era esperado no es igual el resultado, y cada uno tiene sus ventajas según lo que se desea lograr.\n","- En los embeddings de Glove morning está muy relacionado con los saludos y este comportamiento podría no ser deseado.\n","- En los embeddings de Fasttext la similitud entre los dos grupos de palabras elegidas no es tan alta."]},{"cell_type":"markdown","metadata":{"id":"DaEtadJHcVMF"},"source":["### 3 - Operaciones con embeddings (reyes y reinas)"]},{"cell_type":"code","metadata":{"id":"4VQk0TK1dv7g"},"source":["test_words = ['king', 'man', 'woman', 'queen']\n","test_emb = model_fasttext.get_words_embeddings(test_words)\n","\n","# king - main + woman\n","new_queen = test_emb[0] - test_emb[1] + test_emb[2]\n","\n","new_words = test_words + ['new_queen']\n","new_emb = np.append(test_emb, new_queen.reshape(1, -1), axis=0)\n","\n","dist = pairwise.cosine_similarity(new_emb, new_emb)\n","\n","plot_matrix_distance(new_words, dist)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkQ7FiblgssY"},"source":["### 5 - Conclusiones\n","Se puede observar que la similitud entre \"queen\" y \"new_queen\" no es tan alta como uno esperaría y esto depende del generador de embeddings, pero:\n","- Está más cerca \"new_queen\" a \"queen\" de lo que está \"woman\" o \"man\"\n","- Es cierto que \"king\" está más cerca de \"queen\" que \"new_queen\" pero es mutuo.\n","- Están bastante cerca \"man\" y \"woman\" y es por eso que la operación realizada tiene sentido"]},{"cell_type":"markdown","metadata":{"id":"nKNIdgzujnHS"},"source":["### 6 - Jugando con otras operaciones"]},{"cell_type":"code","metadata":{"id":"3UyJVeokhfDh"},"source":["test_words2 = ['anarchy', 'monarchy', 'kingdom', 'crown', 'royal']\n","test_emb2 = model_fasttext.get_words_embeddings(test_words2)\n","\n","# king - man\n","no_king = test_emb[0] - test_emb[1]\n","\n","new_words2 = test_words2 + ['no_king']\n","new_emb2 = np.append(test_emb2, no_king.reshape(1, -1), axis=0)\n","\n","dist2 = pairwise.cosine_similarity(new_emb2, new_emb2)\n","\n","plot_matrix_distance(new_words2, dist2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGsZwv63i4ey"},"source":["# king - royal\n","new_man = test_emb[0] - test_emb2[4]\n","\n","new_words3 = test_words + ['new_man']\n","new_emb3 = np.append(test_emb, new_man.reshape(1, -1), axis=0)\n","\n","dist3 = pairwise.cosine_similarity(new_emb3, new_emb3)\n","\n","plot_matrix_distance(new_words3, dist3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvV6FXiehWCV"},"source":["### 7 - Interpretación gráfica de reyes y reinas\n","Utilizaramos TSNE para representar los embeddings y dos dimensiones y poder compararlos. En una de la mejores formas de reducir la dimensionalidad de embeddings para comparación visual."]},{"cell_type":"code","metadata":{"id":"hO_UrKi7lWi7"},"source":["import numpy as np\n","from sklearn.manifold import TSNE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WpXLW2M9kG50"},"source":["# Visualizar los primeros embeddings calculados, con \"queen\" y \"new_queen\"\n","X = np.asanyarray(new_emb)\n","y = np.asanyarray(new_words)\n","\n","X_embedded = TSNE(n_components=2).fit_transform(X, y)\n","\n","fig = plt.figure(figsize=(16, 9))\n","ax = fig.add_subplot()\n","sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y, palette='bright', ax=ax)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pXYcXfJkhOE"},"source":["# Visualizar todos los embeddings calculados, con \"new_queen\" y \"no_king\"\n","X = np.asanyarray(np.append(new_emb, new_emb2, axis=0))\n","y = np.asanyarray(new_words + new_words2)\n","\n","X_embedded = TSNE(n_components=2).fit_transform(X, y)\n","\n","fig = plt.figure(figsize=(16, 9))\n","ax = fig.add_subplot()\n","sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y, palette='bright', ax=ax)"],"execution_count":null,"outputs":[]}]}
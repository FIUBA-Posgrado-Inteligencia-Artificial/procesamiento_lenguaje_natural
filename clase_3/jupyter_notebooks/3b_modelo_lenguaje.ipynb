{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Predicción de próxima palabra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Objetivo\n",
        "El objetivo es entrenar un modelo de lenguaje basado en arquitectura de redes recurrentes a partir de un corpus de texto. En el transcurso del ejercicio se explorarán técnicas de generación de secuencias y se medirá la calidad de las mismas calculando la perplejidad. Parte del ejercicio consiste en estructurar adecuadamente el dataset para este problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkdPfrQJZdB5"
      },
      "outputs": [],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import platform\n",
        "if os.access('./songs_dataset', os.F_OK) is False:\n",
        "    if os.access('songs_dataset.zip', os.F_OK) is False:\n",
        "        if platform.system() == 'Windows':\n",
        "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
        "        else:\n",
        "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
        "    !unzip -q songs_dataset.zip\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j-3nQ4lZjfb"
      },
      "outputs": [],
      "source": [
        "# Posibles bandas\n",
        "os.listdir(\"./songs_dataset/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb39v3PaZmRH"
      },
      "outputs": [],
      "source": [
        "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
        "df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeMpWpiyG5Ya"
      },
      "source": [
        "Consideraremos que cada secuencia para este dataset es un verso.\n",
        "\n",
        "Una de las primeras decisiones que hay que tomar es el tamaño de contexto de tokens máximo que puede consumir el modelo. Este podría ser un hiperparámetro del problema.\n",
        "\n",
        "Para elegir el tamaño de contexto máximo para este problema se puede explorar el dataset, para ello:\n",
        "- se consideran las palabras como términos.\n",
        "- se segmentará el texto de todos los versos del dataset y ses explorará la cantidad de términos presentes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riT898QlZnmF",
        "outputId": "1705c0de-53b9-446a-93b5-d1523815bcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cantidad de documentos: 1846\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de documentos:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jveeyMVKHn9J"
      },
      "outputs": [],
      "source": [
        "# cada verso lo guardamos en una lista\n",
        "text = list(df.loc[:,0])\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CMu9CX34J8RQ"
      },
      "outputs": [],
      "source": [
        "# segmentamos el texto con la utilidad de Keras\n",
        "segmented_sentences = [text_to_word_sequence(sentence) for sentence in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x35rV7QZH49n"
      },
      "outputs": [],
      "source": [
        "# calculamos la longitud de cada secuencia\n",
        "length_sentences = [len(sentence) for sentence in segmented_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8P8vDXRII4c"
      },
      "outputs": [],
      "source": [
        "# podemos ver su distribución\n",
        "plt.hist(length_sentences,bins=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wumBNwdjJM3j",
        "outputId": "a2438807-eee1-4312-e974-9b7b0d99bbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_context_size: 12\n"
          ]
        }
      ],
      "source": [
        "# a partir de la distribución de longitudes de secuencias elegimos algún criterio\n",
        "# para determinar el máximo tamaño de contexto. En este caso es un percentil, pero\n",
        "# otros criterios también pueden ser válidos con la justificación adecuada.\n",
        "\n",
        "# el -1 es porque el último token será el target\n",
        "max_context_size = int(np.percentile(length_sentences, 90)-1)\n",
        "# max_context_size = int(np.ceil(np.mean(length_sentences))) # criterio de media\n",
        "# max_context_size = int(np.ceil(np.median(length_sentences))) # criterio de mediana\n",
        "print(f'max_context_size: {max_context_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_XKQIpRiLNbg"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer()\n",
        "\n",
        "# El tokenizer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "# El token 0 es reservado y no es asignado. Se utiliza para designar a palabras\n",
        "# fuera del vocabulario aprendido\n",
        "tok.fit_on_texts(segmented_sentences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "tokenized_sentences = tok.texts_to_sequences(segmented_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bUqdwyjLYh8"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b48R11MMZ9SR",
        "outputId": "72610688-7083-4aeb-96a3-b68f0981a3b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_context_size+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cSeqVGyV_wz5"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train, tokenized_sentences_val, _, _ = train_test_split(tokenized_sentences, tokenized_sentences, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmsoPbV6LxcW"
      },
      "source": [
        "Vamos a splitear las oraciones que tienen tamaño mayor al contexto máximo, para generarnos más secuencias de entrenamiento. Este paso puede obviarse si el tamaño de contexto máximo es muy grande."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E5BPO4qPPnNR"
      },
      "outputs": [],
      "source": [
        "tok_sent = []\n",
        "\n",
        "for sent in tokenized_sentences_train:\n",
        "\n",
        "  # si la secuencia tiene más términos que el tamaño de contexto máximo,\n",
        "  # armo varias sub-secuencias de tamaño máximo\n",
        "  if len(sent) > (max_context_size+1):\n",
        "    extra = len(sent)-(max_context_size+1) + 1\n",
        "    for i in range(extra):\n",
        "      tok_sent.append(sent[i:i+max_context_size+1])\n",
        "  else: # si la secuencia tiene menos términos el tamaño de contexto máximo, dejo la secuencia como está\n",
        "    tok_sent.append(sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiEt0AAz_64v",
        "outputId": "eb9b9090-8393-4bf6-d08e-78c48309bba5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1957"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tok_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnwSC7A_LWfd"
      },
      "source": [
        "Ahora aumentamos los datos aprovechando que de una secuencia grande se pueden generar varias más pequeñas:\n",
        "\n",
        "- *La hermosa casa en el prado*\n",
        "- *La hermosa*\n",
        "- *La hermosa casa*\n",
        "- *La hermosa casa en*\n",
        "- *La hermosa casa en el*\n",
        "- *La hermosa casa en el prado*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lWdhVV04htki"
      },
      "outputs": [],
      "source": [
        "tok_sent_augm = []\n",
        "\n",
        "for sent in tok_sent:\n",
        "\n",
        "  # generamos todas las sub-secuencias\n",
        "  subseq = [sent[:i+2] for i in range(len(sent)-1)]\n",
        "  # en esta línea paddeamos al tamaño de contexto máximo\n",
        "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size+1, padding='pre'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXXBya1tnZ8"
      },
      "outputs": [],
      "source": [
        "# finalmente concatenamos todas las secuencias en un único array de numpy\n",
        "train_seqs = np.concatenate(tok_sent_augm, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5Uiflnwt10J",
        "outputId": "98b2d575-e192-4bc6-9870-9062a93f5009"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15325, 13)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_seqs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yprwJHiMBQIS"
      },
      "outputs": [],
      "source": [
        "# y de aquí sacamos las entradas y los targets que consumirá nuestro sistema en\n",
        "# tiempo de entrenamiento\n",
        "X = train_seqs[:,:-1]\n",
        "y = train_seqs[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D9ESMGyB_QD"
      },
      "outputs": [],
      "source": [
        "# Palabras del vocabulario\n",
        "tok.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtzINYjWCMf1",
        "outputId": "679519a5-bdf8-4bcb-8046-55e4dc8b4d34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1628"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spTBxmFQc6h8",
        "outputId": "6a2b0d9e-1de1-477a-dbff-d1f5e1748601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'troubles': 1, 'my': 2, 'seemed': 3, 'all': 4, 'so': 5, 'yesterday': 6, 'far': 7}\n"
          ]
        }
      ],
      "source": [
        "# El índice para cada palabra\n",
        "# El sistema las ordena de las más populares a las menos populares\n",
        "print(tok.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUDkjy80c77h",
        "outputId": "c9d4faa7-e58f-4bc5-ba72-0156c3ff83b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'my': 3, 'troubles': 4, 'all': 2, 'yesterday': 1, 'seemed': 3, 'so': 2, 'far': 1})\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de veces quea aparece cada palabra en cada \"documento\"\n",
        "# (1 documento = 1 caso de entrada)\n",
        "print(tok.word_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzTZRXrrwrvi"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Embedding:\n",
        "# input_seq_len = 3 --> ingreso 3 palabras\n",
        "# input_dim = vocab_size --> 1628 palabras distintas\n",
        "# output_dim = 5 --> crear embeddings de tamaño 3 (tamaño variable y ajustable)\n",
        "model.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64)) # La última capa LSTM no lleva return_sequences\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Predicción de clasificación con softmax\n",
        "# La salida vuelve al espacio de 1628 palabras posibles\n",
        "model.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Clasificación multiple categórica --> loss = categorical_crossentropy\n",
        "# notar que usamos la versión Sparse para utilizar sólo índices en lugar de OHE\n",
        "model.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWK3z85sQfUe"
      },
      "source": [
        "Dado que por el momento no hay implementaciones adecuadas de la perplejidad que puedan operar en tiempo de entrenamiento, armaremos un Callback *ad-hoc* que la calcule en cada epoch.\n",
        "\n",
        "**Nota**: un Callback es una rutina gatillada por algún evento, son muy útiles para relevar datos en diferentes momentos del desarrollo del modelo. En este caso queremos hacer un cálculo cada vez que termina una epoch de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(len_seq)])\n",
        "        self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "        self.info.append((count,count+len_seq))\n",
        "        count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        print(f'\\n mean perplexity: {np.mean(scores)} \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQq1PHDkxDvN"
      },
      "outputs": [],
      "source": [
        "# fiteamos, nótese el agregado del callback con su inicialización. El batch_size lo podemos seleccionar a mano\n",
        "# en general, mientras más grande mejor.\n",
        "hist = model.fit(X, y, epochs=50, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_orBXOrCsNn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Fg_BsxJe6"
      },
      "source": [
        "\n",
        "### Predicción de próxima palabra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy_AXWQWzeeE"
      },
      "outputs": [],
      "source": [
        "# Keras pad_sequences\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "# Si la secuencia de entrada supera al input_seq_len (3) se trunca\n",
        "# Si la secuencia es más corta se agregna ceros al comienzo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBvKHFPmzpy2"
      },
      "outputs": [],
      "source": [
        "# Se utilizará gradio para ensayar el modelo\n",
        "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
        "# https://gradio.app/\n",
        "\n",
        "!pip install -q gradio "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNyBykvhzs7-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = tok.texts_to_sequences([human_text])[0]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "\n",
        "    # Debemos buscar en el vocabulario la palabra\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    for word, index in tok.word_index.items():\n",
        "        if index == y_hat:\n",
        "            out_word = word\n",
        "            break\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + ' ' + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        # Debemos buscar en el vocabulario la palabra\n",
        "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == y_hat:\n",
        "                out_word = word\n",
        "                break\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += ' ' + out_word\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFqRC5pxzqS"
      },
      "outputs": [],
      "source": [
        "input_text='hey jude don\\'t'\n",
        "\n",
        "generate_seq(model, tok, input_text, max_length=max_context_size, n_words=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = tok.texts_to_sequences([text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return tok.sequences_to_texts([seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp=1):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  # idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = np.squeeze(model.predict(encoded))\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = np.squeeze(model.predict(input_update))\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens)\n",
        "\n",
        "    return history_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeLqAoOYW1Hm"
      },
      "outputs": [],
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model,num_beams=10,num_words=6,input=\"when i find myself in times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S3_I3S1W1Hm"
      },
      "outputs": [],
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2SHmXbgxQH9"
      },
      "source": [
        "### Conclusiones\n",
        "El modelo entrenado tuvo un muy mail desempeño en el entrenamiento además de overfitting. Cuestiones que podrían mejorarse:\n",
        "- Agregar más capas o neuronaes\n",
        "- Incrementar la cantidad de épocas\n",
        "- Agregar BRNN\n",
        "\n",
        "Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquí en más utilizaremos más datos, embeddings pre-enternados o modelos pre-entrenados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_LlqmtEW1Hn"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

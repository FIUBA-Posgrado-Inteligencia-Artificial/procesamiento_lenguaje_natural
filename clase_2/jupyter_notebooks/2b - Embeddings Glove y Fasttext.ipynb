{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Ua7AcJXvEW"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Word Embeddings con Glove y Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V64c1mLJeQFq"
   },
   "source": [
    "Este código permite levantar los embeddings de fastText y de GloVe y poder utilizarlos para calcular la distancia entre palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U2K2RERweIY"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoahGjfGX1vN"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/u/0/uc?id=1Qi1r-u5lsEsNqRSxLrpNOqQ3B_ufltCa'\n",
    "output = 'fasttext.pkl'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvaLePMwpBjV"
   },
   "outputs": [],
   "source": [
    "# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n",
    "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
    "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
    "# Download fastText Embeddings \n",
    "#!curl -L -o 'fasttext.pkl' 'https://drive.google.com/u/0/uc?id=1Qi1r-u5lsEsNqRSxLrpNOqQ3B_ufltCa&export=download&confirm=t'\n",
    "\n",
    "# Download GloVe Embeddings\n",
    "#!curl -L -o 'gloveembedding.pkl' 'https://drive.google.com/u/0/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0RVEeHfzbg3"
   },
   "outputs": [],
   "source": [
    "# De donde se obtuvieron estos embeddings:\n",
    "\n",
    "# Link de donde se dscargo los embeddings de GloVe (de ese zip solo se sube el embedding de 50d)\n",
    "# http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# Link de descarga de los embeddings 300 de fastText\n",
    "# https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCuwnSJn7WPF"
   },
   "outputs": [],
   "source": [
    "# Paper de GloVe\n",
    "# https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "# Papers de FastText\n",
    "# https://arxiv.org/pdf/1607.04606.pdf\n",
    "# https://arxiv.org/abs/1607.01759\n",
    "# https://arxiv.org/abs/1612.03651"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTNWSWVWaBno"
   },
   "source": [
    "### 1 - Ensayar los embeddings de Glove y Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9HDjEUNd4ff"
   },
   "outputs": [],
   "source": [
    "# Acá definimos una clase que nos va a permitir manejar los embeddings de GloVe\n",
    "# y fastText con la misma interfase.\n",
    "# Incluye cómo cargar los embeddings a partir de sus pickles \n",
    "# (formato de datos serializados de Python) y guardarlos\n",
    "# funciones para obtener términos dados índices y viceversa\n",
    "\n",
    "class WordsEmbeddings(object):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        # load the embeddings\n",
    "        words_embedding_pkl = Path(self.PKL_PATH)\n",
    "        if not words_embedding_pkl.is_file():\n",
    "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
    "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
    "            embeddings = self.convert_model_to_pickle()\n",
    "        else:\n",
    "            embeddings = self.load_model_from_pickle()\n",
    "        self.embeddings = embeddings\n",
    "        # build the vocabulary hashmap\n",
    "        index = np.arange(self.embeddings.shape[0])\n",
    "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
    "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
    "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
    "\n",
    "    def get_words_embeddings(self, words):\n",
    "        words_idxs = self.words2idxs(words)\n",
    "        return self.embeddings[words_idxs]['embedding']\n",
    "\n",
    "    def words2idxs(self, words):\n",
    "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
    "\n",
    "    def idxs2words(self, idxs):\n",
    "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
    "\n",
    "    def load_model_from_pickle(self):\n",
    "        self.logger.debug(\n",
    "            'loading words embeddings from pickle {}'.format(\n",
    "                self.PKL_PATH\n",
    "            )\n",
    "        )\n",
    "        max_bytes = 2**28 - 1 # 256MB\n",
    "        bytes_in = bytearray(0)\n",
    "        input_size = os.path.getsize(self.PKL_PATH)\n",
    "        with open(self.PKL_PATH, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        embeddings = pickle.loads(bytes_in)\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "    def convert_model_to_pickle(self):\n",
    "        # create a numpy strctured array:\n",
    "        # word     embedding\n",
    "        # U50      np.float32[]\n",
    "        # word_1   a, b, c\n",
    "        # word_2   d, e, f\n",
    "        # ...\n",
    "        # word_n   g, h, i\n",
    "        self.logger.debug(\n",
    "            'converting and loading words embeddings from text file {}'.format(\n",
    "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
    "            )\n",
    "        )\n",
    "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
    "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
    "        structure = np.dtype(structure)\n",
    "        # load numpy array from disk using a generator\n",
    "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
    "            embeddings_gen = (\n",
    "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
    "                if len(line.split()[1:]) == self.N_FEATURES\n",
    "            )\n",
    "            embeddings = np.fromiter(embeddings_gen, structure)\n",
    "        # add a null embedding\n",
    "        null_embedding = np.array(\n",
    "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
    "            dtype=structure\n",
    "        )\n",
    "        embeddings = np.concatenate([embeddings, null_embedding])\n",
    "        # dump numpy array to disk using pickle\n",
    "        max_bytes = 2**28 - 1 # # 256MB\n",
    "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(self.PKL_PATH, 'wb') as f_out:\n",
    "            for idx in range(0, len(bytes_out), max_bytes):\n",
    "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "# Armamos clases particulares para manejar los embeddings de Glove y Fasttext\n",
    "# que heredan de la clase anterior WordsEmbeddings\n",
    "class GloveEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
    "    PKL_PATH = 'gloveembedding.pkl'\n",
    "    N_FEATURES = 50\n",
    "    WORD_MAX_SIZE = 60\n",
    "\n",
    "\n",
    "class FasttextEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
    "    PKL_PATH = 'fasttext.pkl'\n",
    "    N_FEATURES = 300\n",
    "    WORD_MAX_SIZE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQKIDrCWfuxV"
   },
   "outputs": [],
   "source": [
    "# Armamos una función para graficar la matriz de similaridad\n",
    "def plot_matrix_distance(words, dist):\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_subplot()\n",
    "    sns.heatmap(dist, xticklabels=words, yticklabels=words, \n",
    "                annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, mask=np.triu(dist))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCBmo0npei9x"
   },
   "outputs": [],
   "source": [
    "# Ojo que si usan scipy la distancia coseno no es la similitud coseno (esta \"negada\")\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "def embeddings_matrix_distance(model, words):\n",
    "    print(\"Cantidad de palabras:\", len(words))\n",
    "    emb = model.get_words_embeddings(words)\n",
    "    print(\"Dimensiones de los embeddings:\", emb.shape)\n",
    "    dist = pairwise.cosine_similarity(emb, emb)\n",
    "    plot_matrix_distance(words, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiCtPUYirc1t"
   },
   "outputs": [],
   "source": [
    "# Instanciamos embedding de fasttext\n",
    "# Puede demorar unos segundos porque tiene que cargar en memoria los archivos de embeddings\n",
    "model_fasttext = FasttextEmbeddings()\n",
    "words_fasttext=[model_fasttext.embeddings[i][0] for i in range(model_fasttext.embeddings.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de palabras en fasttext\n",
    "model_fasttext.embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MlZWL1KaQq2"
   },
   "outputs": [],
   "source": [
    "# Palabras a ensayar\n",
    "# Algunas relativas con saludos y otras con dispositivos\n",
    "words = [\"hi\", \"hello\", \"bye\", \"goodbye\", \"morning\", \"computer\", \"machine\", \"laptop\", \"device\", \"printer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hfg7_QxaYBf"
   },
   "outputs": [],
   "source": [
    "embeddings_matrix_distance(model_fasttext, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VQk0TK1dv7g"
   },
   "outputs": [],
   "source": [
    "# probamos un test de analogía\n",
    "test_words = ['king', 'man', 'woman', 'queen']\n",
    "test_emb = model_fasttext.get_words_embeddings(test_words)\n",
    "\n",
    "# king - man + woman\n",
    "new_queen = test_emb[0] - test_emb[1] + test_emb[2]\n",
    "\n",
    "new_words = test_words + ['new_queen']\n",
    "new_emb = np.append(test_emb, new_queen.reshape(1, -1), axis=0)\n",
    "\n",
    "dist = pairwise.cosine_similarity(new_emb, new_emb)\n",
    "\n",
    "plot_matrix_distance(new_words, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mas allá de las palabras propuestas, podemos ver cuál sería la palabra \n",
    "# cuyo embedding es más similar al de 'new_queen'\n",
    "# para ello vamos a disponibilizar los embeddings como un numpy array\n",
    "embeddings_fasttext = []\n",
    "for idx,emb in enumerate(model_fasttext.embeddings):\n",
    "  embeddings_fasttext.append(emb[1])\n",
    "embeddings_fasttext = np.array(embeddings_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos la similaridad de 'new_queen' contra todos los vectores\n",
    "# puede tardar unos segundos\n",
    "similarities = np.squeeze(pairwise.cosine_similarity(embeddings_fasttext,np.array([new_queen])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos cuáles son los 10 vectores más similares\n",
    "args_sorted = np.argsort(similarities)[::-1][:10]\n",
    "for arg in args_sorted:\n",
    "  print(f'{words_fasttext[arg]}: {similarities[arg]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UyJVeokhfDh"
   },
   "outputs": [],
   "source": [
    "test_words2 = ['anarchy', 'monarchy', 'kingdom', 'crown', 'royal']\n",
    "test_emb2 = model_fasttext.get_words_embeddings(test_words2)\n",
    "\n",
    "# king - man\n",
    "no_king = test_emb[0] - test_emb[1]\n",
    "\n",
    "new_words2 = test_words2 + ['no_king']\n",
    "new_emb2 = np.append(test_emb2, no_king.reshape(1, -1), axis=0)\n",
    "\n",
    "dist2 = pairwise.cosine_similarity(new_emb2, new_emb2)\n",
    "\n",
    "plot_matrix_distance(new_words2, dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGsZwv63i4ey"
   },
   "outputs": [],
   "source": [
    "# king - royal\n",
    "new_man = test_emb[0] - test_emb2[4]\n",
    "\n",
    "new_words3 = test_words + ['new_man']\n",
    "new_emb3 = np.append(test_emb, new_man.reshape(1, -1), axis=0)\n",
    "\n",
    "dist3 = pairwise.cosine_similarity(new_emb3, new_emb3)\n",
    "\n",
    "plot_matrix_distance(new_words3, dist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/u/0/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94'\n",
    "output = 'gloveembedding.pkl'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciamos embeddings de GloVe\n",
    "# antes liberamos la RAM de los embeddings de fasttext \n",
    "# para tener disponible suficiente RAM para hacer operaciones\n",
    "del(model_fasttext)\n",
    "model_glove = GloveEmbeddings()\n",
    "words_glove=[model_glove.embeddings[i][0] for i in range(model_glove.embeddings.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de palabras en GloVe\n",
    "model_glove.embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnrchHWMZ5-p"
   },
   "outputs": [],
   "source": [
    "embeddings_matrix_distance(model_glove, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOxqbPg7b52U"
   },
   "source": [
    "Diferentes embeddings capturan diferentes relaciones semánticas y en distinto grado:\n",
    "- En los embeddings de Glove morning está muy relacionado con los saludos y este comportamiento podría no ser deseado.\n",
    "- En los embeddings de Fasttext la similitud entre los dos grupos de palabras elegidas no es tan alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

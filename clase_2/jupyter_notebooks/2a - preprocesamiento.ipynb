{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5hxxkdAQJg"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Preprocesamiento con NLTK y Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCED1hh-Ioyf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOa4JPSCJ29"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdW55pJin1rt"
   },
   "outputs": [],
   "source": [
    "simple_text = \"if she leaves now she might miss something important!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIO7b8GjAC17"
   },
   "outputs": [],
   "source": [
    "large_text = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVHxBRNzCMOS"
   },
   "source": [
    "### 1 - Preprocesamiento con NLTK\n",
    "- Cada documento transformarlo en una lista de términos\n",
    "- Armar un vector de términos no repetidos de todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM-lmmsFnC6X"
   },
   "outputs": [],
   "source": [
    "# NLTK cuenta con una extensiva documentación \n",
    "# que vale la pena consultar\n",
    "# https://www.nltk.org\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar tokenizador punkt\n",
    "nltk.download(\"punkt\")\n",
    "# Descargar la red de semántica del inglés WordNet\n",
    "# Es una extensa red semántica que puede usarse (entre otras cosas)\n",
    "# para hacer POS tagging o lematizar.\n",
    "nltk.download(\"wordnet\")\n",
    "# Descargar diccionario de stopwords del inglés\n",
    "nltk.download('stopwords')\n",
    "# Para usar NLTK 3.6.6 o superior es necesario instalar OMW 1.4 \n",
    "# (Open Multilingual WordNet)\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soL5T-UDsZ20"
   },
   "outputs": [],
   "source": [
    "simple_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Er9fvFonfT1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciar el derivador, NLTK provee varios derivadores para elegir:\n",
    "# https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuEob1D6nEPK"
   },
   "outputs": [],
   "source": [
    "# Instanciar el lematizador\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GE9pq3dMod6Y"
   },
   "outputs": [],
   "source": [
    "# Extraer los tokens de un doc\n",
    "# estos no van a ser los tokens finales, sino que serán procesados\n",
    "# aplicando lematización, filtrando, etc...\n",
    "tokens = word_tokenize(simple_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSdedQvVM-wN"
   },
   "outputs": [],
   "source": [
    "# Transformar los tokens a sus respectivas palabras derivadas\n",
    "# Stemming\n",
    "nltk_stemedList = []\n",
    "for word in tokens:\n",
    "    nltk_stemedList.append(p_stemmer.stem(word))\n",
    "print(\"Stemming:\", nltk_stemedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV3-wVBSNNaA"
   },
   "outputs": [],
   "source": [
    "# Transformar los tokens a sus respectivas palabras raiz\n",
    "# Lemmatization\n",
    "nltk_lemmaList = []\n",
    "for word in tokens:\n",
    "    nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "print(\"Lemmatization:\", nltk_lemmaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFzuIwPZuNqo"
   },
   "outputs": [],
   "source": [
    "# Miremos el conjunto de los signos de puntuación\n",
    "# que contiene la librería `string` de Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U47nxm8ZNiIr"
   },
   "outputs": [],
   "source": [
    "# Quitar los signos de puntuacion\n",
    "nltk_punctuation = [w for w in nltk_lemmaList if w not in string.punctuation]\n",
    "print(\"Punctuation filter:\", nltk_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A71y7Ecsun-u"
   },
   "outputs": [],
   "source": [
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "len(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImlO-N45OuKG"
   },
   "outputs": [],
   "source": [
    "# Filtrar stopwords\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_sentence = [w for w in nltk_punctuation if w not in nltk_stop_words]\n",
    "print(\"Stop words filter:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NrPtt2OmWBv"
   },
   "source": [
    "### 2 - Proceso completo con NLTK\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZqTOZzDI7uv"
   },
   "outputs": [],
   "source": [
    "def nltk_process(text):\n",
    "    # Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "      \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"Lemmatization\")\n",
    "    print(nltk_lemmaList)\n",
    "\n",
    "    # Stop words\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
    "\n",
    "    # Filter Punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & Punctuation\")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZdiop6IJpZN"
   },
   "outputs": [],
   "source": [
    "nltk_text = nltk_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M4F0ll1msUY"
   },
   "source": [
    "### 3 - Proceso completo con spaCy\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r57e9b9Omwnh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Cargar pipeline de preprocesamiento de inglés\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization & lemmatization\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    # Stop words\n",
    "    filtered_sentence =[]\n",
    "    for word in lemma_list:\n",
    "        # word es un string, para recuperar la información de los objetos de SpaCy\n",
    "        # necesitamos usar el string para pasar a un lexema, el objeto de SpaCy\n",
    "        # que para cada término contiene la información del preprocesamiento\n",
    "        # (se podría también directamente filtrar stopwords en el paso de lematización)\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    # Filter punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x_iKHu1pKBE"
   },
   "outputs": [],
   "source": [
    "spacy_text = spacy_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txMZ7lR-pvHB"
   },
   "source": [
    "### 4 - Conclusiones\n",
    "- NLTK no pasa a minúsculas el texto por su cuenta\n",
    "- spacy algunas palabras las reemplaza por su Tag (como \"'\")\n",
    "- spacy descompone palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te3GgSNzpbGq"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable(['NLTK', 'spaCy'])\n",
    "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
    "    table.add_row([nltk_word, spacy_word])\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a - preprocesamiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

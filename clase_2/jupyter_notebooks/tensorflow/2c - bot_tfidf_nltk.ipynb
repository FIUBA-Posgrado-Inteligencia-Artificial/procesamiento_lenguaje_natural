{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5hxxkdAQJg"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Sistema de obtención de información con NLTK utilizando un corpus de wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCED1hh-Ioyf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import re # Regular Expressions (regex)\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Para leer y parsear el texto en HTML de wikipedia\n",
    "import bs4 as bs\n",
    "\n",
    "import nltk\n",
    "# Descargar el diccionario\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOa4JPSCJ29"
   },
   "source": [
    "### Datos\n",
    "Se consumirán los datos del artículo de wikipedia sobre el deporte \"Tennis\" en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIO7b8GjAC17"
   },
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Tennis')\n",
    "raw_html = raw_html.read()\n",
    "\n",
    "# Parsear artículo, 'lxml' es el parser a utilizar\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
    "# y tenerlos disponible como lista\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text\n",
    "\n",
    "article_text = article_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUH30a1_rOkS"
   },
   "outputs": [],
   "source": [
    "# Demos un vistazo\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtGLJjt6rQhK"
   },
   "outputs": [],
   "source": [
    "print(\"Cantidad de caracteres en la nota:\", len(article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVHxBRNzCMOS"
   },
   "source": [
    "### 2 - Preprocesamiento\n",
    "- Remover caracteres especiales\n",
    "- Quitar espacios o saltos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnEUTD1Erl1N"
   },
   "outputs": [],
   "source": [
    "# Repaso de regex:\n",
    "# https://docs.python.org/3/library/re.html\n",
    "\n",
    "# Para practicar regex:\n",
    "# https://regex101.com/\n",
    "\n",
    "# el inicio con 'r' antes de cada string indica que se interprete como raw string\n",
    "# '\\n' es interpretado por Python como salto de linea\n",
    "# r'\\n' es interpretado por Python como el string formado por dos caracteres: \n",
    "#  backslash y n\n",
    "\n",
    "# substituir con regex con espacio vacío:\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', article_text) # substituir los números entre corchetes\n",
    "# (notar que los corchetes son interpretados literalmente por los backlsash)\n",
    "text = re.sub(r'\\s+', ' ', text) # substituir más de un caracter de espacio, salto de línea o tabulación\n",
    "\n",
    "# probar en regex101 con los patrones anteriores:\n",
    "# 'Hola [1], [], [ estoy bien   [123]. [12sss]. OK!   .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7ycrAMYrn66"
   },
   "outputs": [],
   "source": [
    "# Demos un vistazo\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA5F0s4UsMpf"
   },
   "outputs": [],
   "source": [
    "print(\"Cantidad de caracteres en el texto:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKNcDGcisajf"
   },
   "source": [
    "### 3 - Dividir el texto en sentencias y en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reXBOFQ7sdlB"
   },
   "outputs": [],
   "source": [
    "corpus = nltk.sent_tokenize(text) # divide en oraciones\n",
    "words = nltk.word_tokenize(text) # divide en términos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5GloV9fsi6o"
   },
   "outputs": [],
   "source": [
    "# Demos un vistazo\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmQ7nkvvsi0i"
   },
   "outputs": [],
   "source": [
    "# Demos un vistazo\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXPWNkKfEvDZ"
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulario:\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlYKyb3OtDse"
   },
   "source": [
    "### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n",
    "- Lematizar los tokens de la oración\n",
    "- Quitar símbolos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afPok8pstPOx"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# ord() nos da el código Unicode para un caracter dado\n",
    "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
    "\n",
    "def get_processed_text(document):\n",
    "    # 1 - reduce el texto a mínuscula (string.lower())\n",
    "    # 2 - quitar los simbolos de puntuacion (string.translate())\n",
    "    # 3 - realiza la tokenización (nltk.word_tokenize)\n",
    "    # 4 - realiza la lematización (nuestra función perform_lemmatization)\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl8r6d9ZuyR9"
   },
   "source": [
    "### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus del artículo de wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRYFHcBfk2Gt"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def generate_response(user_input, corpus):\n",
    "    response = ''\n",
    "    # Sumar al corpus la pregunta del usuario para calcular\n",
    "    # su cercania con otros documentos/sentencias\n",
    "    # la entrada del usuario se usa para tokenizar y vectorizar\n",
    "    corpus.append(user_input)\n",
    "\n",
    "    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n",
    "    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
    "\n",
    "    # Crear los vectores a partir del corpus\n",
    "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n",
    "    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "\n",
    "    # Obtener el índice del vector más cercano a nuestra oración\n",
    "    # --> descartando la similitud contra nuestor vector propio\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    if vector_matched == 0: # si la similaridad coseno fue nula (ningún término en común)\n",
    "        response = \"I am sorry, I could not understand you\"\n",
    "    else:\n",
    "        response = corpus[similar_sentence_number] # obtener el documento del corpus más similar\n",
    "    \n",
    "    corpus.remove(user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK-BuXPBybSp"
   },
   "source": [
    "### 6 - Ensayar el sistema\n",
    "El sistema intentará encontrar la parte del artículo que más se relaciona con nuestro texto de entrada. Sugerencias a ensayar:\n",
    "- Grand slam\n",
    "- tournaments\n",
    "- nadal\n",
    "- artificial intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2X4j8XyydSb"
   },
   "outputs": [],
   "source": [
    "# Se utilizará gradio para ensayar el bot\n",
    "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
    "# https://gradio.app/\n",
    "import sys\n",
    "!{sys.executable} -m pip install gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZv5MiVzynG1"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def bot_response(human_text):\n",
    "    print(\"Q:\", human_text)    \n",
    "    resp = generate_response(human_text.lower(), corpus)\n",
    "    print(\"A:\", resp)\n",
    "    return resp\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=bot_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\",\n",
    "    layout=\"vertical\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alumno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tomar un ejemplo de los bots utilizados (uno de los dos) y construir el propio.\n",
    "- Sacar conclusiones de los resultados.\n",
    "\n",
    "__IMPORTANTE__: Recuerde para la entrega del ejercicio debe quedar registrado en el colab las preguntas y las respuestas del BOT para que podamos evaluar el desempeño final."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2c - bot_tfidf_nltk.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

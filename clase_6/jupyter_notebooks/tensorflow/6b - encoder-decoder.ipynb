{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfa39F4lsLf3"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## LSTM encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqO0PRcFsPTe"
   },
   "source": [
    "### 1 - Datos\n",
    "El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes LSTM. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n",
    "[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cq3YXak9sGHd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, SimpleRNN\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9aNLZBDtA5J"
   },
   "outputs": [],
   "source": [
    "# Generar datos sintéticos\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "# En ambos casos \"X\" e \"y\" son vectores de números de 5 en 5\n",
    "X = [x for x in range(5, 301, 5)]\n",
    "y = [x+15 for x in X]\n",
    "\n",
    "print(f\"datos X (len={len(X)}):\", X)\n",
    "print(f\"datos y (len={len(y)}):\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ACkrI0GtEAM"
   },
   "outputs": [],
   "source": [
    "# Se desea agrupar los datos de a 3 elementos\n",
    "X = np.array(X).reshape(len(X)//3, 3, 1)\n",
    "y = np.array(y).reshape(len(y)//3, 3, 1)\n",
    "print(\"datos X[0:2]:\", X[0:2])\n",
    "print(\"datos y[0:2]:\", y[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajykLcJ8tFfN"
   },
   "outputs": [],
   "source": [
    "# Verificamos que la secuencia entrada es igual a la secuencia de salida\n",
    "# en cuanto a dimensiones\n",
    "# Tendremos:\n",
    "#  --> veinte grupos de datos (rows) (20)\n",
    "#  --> cada grupo compuesto por tres elementos (3)\n",
    "#  --> cada elemento representado en una sola dimensión (1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZz9Zsvy5ilc"
   },
   "source": [
    "### 2 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCTu6NElI1ge"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Armar el \"corpus\" de números\n",
    "data = np.append(X, y)\n",
    "\n",
    "# Identificar el vocabulario del corpus (todos los números únicos)\n",
    "labels = list(np.unique(data))\n",
    "\n",
    "# Al vocabulario (labels) agregarle el token de inicio de secuencia\n",
    "# Se utiliza el número \"0\" para el <sos>, aprovechando que el \"0\"\n",
    "# no se encuentra en el vocabulario\n",
    "# En este ejemplo todavía no vamos a usar un token especial de <eos>\n",
    "# ya que siempre queremos inferir una secuencia de tamaño fijo \n",
    "# a la salida del decoder\n",
    "labels = [0] + labels # <sos> + vocabulario\n",
    "\n",
    "# LabelEncoder transforma el dato al token correspondiente\n",
    "# Ahora cada número cumple el rol de un término de un corpus\n",
    "# y tendrá su índice asociado después de pasar por una tokenización\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J86H4qb6NEQp"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Preparar datos para consumir por la layers LSTM\n",
    "# Por cada dato de X e Y se genera su contraparte oneHotEncoding\n",
    "# X1 equivale a X, es la secuencia de entrada\n",
    "# X2 es una secuencia equivalente a y pero sin el último elemento y \n",
    "# teniendo al token <sos> como primer elemento. Es la secuencia de \"estado anterior\"\n",
    "# target equivale a Y como OneHotEncoding, la predicción completa con el último elemento\n",
    "# a la salida del decoder\n",
    "\n",
    "def get_dataset(X, Y, label_encoder):\n",
    "\n",
    "    cardinality = len(label_encoder.classes_)\n",
    "    print(\"Number of features/cardinality:\", cardinality)\n",
    "\n",
    "    X1, X2, target = list(), list(), list()\n",
    "    for x, y in zip(X, Y):\n",
    "        input = list(label_encoder.transform(x.reshape(-1)))\n",
    "        output = list(label_encoder.transform(y.reshape(-1)))\n",
    "        # Crear la entrada del \"último estado\" de salida\n",
    "        # que es la salida sin el último elemento, que es el que el modelo\n",
    "        # debe predecir\n",
    "        output_in = [0] + output[:-1]\n",
    "\n",
    "        # transformar\n",
    "        input_encoded = to_categorical(input, num_classes=cardinality)\n",
    "        output_encoded = to_categorical(output, num_classes=cardinality)\n",
    "        output_in_encoded = to_categorical(output_in, num_classes=cardinality)\n",
    "        \n",
    "        # almacenar\n",
    "        X1.append(input_encoded)\n",
    "        X2.append(output_in_encoded)\n",
    "        target.append(output_encoded)\n",
    "    return np.array(X1), np.array(X2), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO2sS3ntkeEf"
   },
   "outputs": [],
   "source": [
    "X1, X2, target = get_dataset(X, y, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu4pTKAm8UWQ"
   },
   "source": [
    "El oneHotEncoding tiene una diemsión de -> vocabulario + tokens_especiales:\n",
    "- 63 números únicos (vocab)\n",
    "- 1 token especial ( \\<sos> como id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXEI1Rfkgngt"
   },
   "outputs": [],
   "source": [
    "# El vector de salida tiene 3 dimensiones:\n",
    "# primera dimensión es la cantidad de \"rows\" del dataset\n",
    "# segunda dimensión es el tamaño de la sequencia de entrada/salida\n",
    "# tercera dimensión es la dimensionalidad del vector oneHotEncoding (cardinalidad)\n",
    "\n",
    "print(\"X1 shape:\", X1.shape)\n",
    "print(\"X2 shape:\", X2.shape)\n",
    "print(\"target shape:\", target.shape)\n",
    "\n",
    "in_features = X1.shape[-1]\n",
    "output_features = target.shape[-1]\n",
    "print(\"Number of input_features\", in_features)\n",
    "print(\"Number of output_features\", output_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vKbhjtIwPgM"
   },
   "source": [
    "### 3 - Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_urD1qO2kOx"
   },
   "outputs": [],
   "source": [
    "# El código a continuación se puede utilizar para cualquier encoder-decoder\n",
    "# que se desee construir con LSTM en el futuro\n",
    "# y es posible cambiar la layer LSTM por otra (GRU, CNN, Attention, etc)\n",
    "# Fuente:\n",
    "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "# https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    '''\n",
    "    Args:\n",
    "        n_input: The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n",
    "        n_output: The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n",
    "        n_units: Size of recurrent layers to use in the encoder and decoder models, e.g. 128 or 256.\n",
    "    output:\n",
    "        train: Model that can be trained given source, target, and shifted target sequences.\n",
    "        inference_encoder: Encoder model used when making a prediction for a new source sequence.\n",
    "        inference_decoder Decoder model use when making a prediction for a new source sequence.\n",
    "    '''\n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHtV5z_wGOwj"
   },
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = define_models(in_features, output_features, n_units=128)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"Adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8PlXsXeuGCT"
   },
   "source": [
    "Conclusiones hasta ahora sobre el modelo:\n",
    "- Nunca se especificó en la layer de entrada cuantos elementos recibirá el sistema, es por eso que el modelo se creó para aceptar una cantidad dinámica de batch y size de elementos de 64 dimensiones --> None, None, 64\n",
    "- El primer None corresponde al batch dinámico\n",
    "- El segundo None corresponde al la cantidad de elementos dinámico\n",
    "\n",
    "¿Cómo sabe cuantos elementos deberá esperar el sistema?\n",
    "- En este caso siempre entregaremos 3 elementos al encoder y esperaremos 3 elementos del decoder.\n",
    "- En un siguiente notebook de seq2seq utilizaremos tokens especiales para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ljAyiBbG10U"
   },
   "outputs": [],
   "source": [
    "# Modelo completo (encoder+decoder) para poder entrenar\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1Wc1pnhIKJ6"
   },
   "outputs": [],
   "source": [
    "# Modelo sólo encoder\n",
    "plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_xanat4INez"
   },
   "outputs": [],
   "source": [
    "# Modelo sólo decoder (para realizar inferencia)\n",
    "plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnlIx1Vezjwc"
   },
   "outputs": [],
   "source": [
    "hist = model.fit([X1, X2], target, epochs=150, validation_split=0.2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVz1uug_zu2J"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist.history['loss']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['loss'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['val_loss'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance medida en términos de la loss empeora conforme se avanza con el entrenamiento\n",
    "Esto es esperable porque en el conjunto de validación se testea sobre token que nunca se usaron en el entrenamiento.\n",
    "Este análisis no tiene mucho valor para este ejemplo didáctico que se enfoca solamente en cómo construir las arquitecturas de encoder y decoder para modelos de secuencia a secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71XeCtfYmOFx"
   },
   "outputs": [],
   "source": [
    "# con esta función pasamos del índice del token al término original\n",
    "def one_hot_decode(encoded_seq, label_encoder):\n",
    "    idx = [np.argmax(vector) for vector in encoded_seq]\n",
    "    return label_encoder.inverse_transform(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR1gKJN2m2Fw"
   },
   "outputs": [],
   "source": [
    "# Ensayo\n",
    "x_test = [20, 25, 30]\n",
    "y_test = [x+15 for x in x_test]\n",
    "\n",
    "print(\"y_test:\", y_test)\n",
    "\n",
    "# Transformar los datos a oneHotEncoding\n",
    "X1_test, X2_test, target_test = get_dataset(np.array([x_test]), np.array([y_test]), label_encoder)\n",
    "\n",
    "print(\"X1_test shape:\", X1_test.shape)\n",
    "print(\"X2_test shape:\", X2_test.shape)\n",
    "print(\"target_test shape:\", target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kluBZJGMm3LF"
   },
   "outputs": [],
   "source": [
    "# Cuando quiera por ejemplo recuperar target_test a y_test utilizo:\n",
    "one_hot_decode(target_test[0], label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4_UQshIzw7o"
   },
   "outputs": [],
   "source": [
    "cardinality = len(label_encoder.classes_)\n",
    "\n",
    "# encode\n",
    "# Se transforma la sequencia de entrada a los estados \"h\" y \"c\" de la LSTM\n",
    "# para enviar la primera vez al decoder\n",
    "state = encoder_model.predict(X1_test)\n",
    "# start of sequence input --> la primera secuencia de salida-entrada (output_in)\n",
    "# comienza con el ID cero (el ID seleccionado para <sos>)\n",
    "output_in = to_categorical(0, num_classes=cardinality)\n",
    "target_seq = np.array(output_in).reshape(1, 1, cardinality)\n",
    "print(\"target_seq shape:\", target_seq.shape)\n",
    "\n",
    "# Vector de predicción\n",
    "output = list()\n",
    "\n",
    "# Aquí empieza la parte auto-regresiva de la inferencia\n",
    "for t in range(3):\n",
    "    # Predicción del próximo elemento\n",
    "    print(\"Qué recibe como elemento anterior:\", one_hot_decode(target_seq[0], label_encoder))\n",
    "    y_hat, h, c = decoder_model.predict([target_seq] + state)\n",
    "    \n",
    "    # Almacenar la predicción\n",
    "    output.append(y_hat[0])\n",
    "    print(\"Qué saca como elemento nuevo o prediccion:\", one_hot_decode(y_hat[0], label_encoder))\n",
    "    # Actualizar los estados dada la última prediccion\n",
    "    state = [h, c]\n",
    "    \n",
    "    # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
    "    target_seq = y_hat\n",
    "\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_hat:\", one_hot_decode(output, label_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7MD_XS2oKUj"
   },
   "outputs": [],
   "source": [
    "# Podría si quisiera usar el \"evaluate\" de Keras pero ahí si necesitaré\n",
    "# de X2:\n",
    "model.evaluate([X1_test, X2_test], [target_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkOjSJweqdF8"
   },
   "source": [
    "### 4 - Conclusión\n",
    "A primera vista parece muy compleja la estructura del encoder-decoder, pero funciona igual en cualquier disciplina de deeplearning:\n",
    "- En visión para transferencia de estilo, generación de imagenes, etc.\n",
    "- En NLP desde LSTM hasta Attention y transformers\n",
    "\n",
    "Hay que pensar el encoder como el generador del \"espacio latente\". Luego el decoder necesita el espacio latente que representa a la setencia de entrada, la realimentación de los valores de salida del decoder y los estados internos de la LSTM para pasar a la siguiente inferencia hasta concluir la secuencia de salida."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
